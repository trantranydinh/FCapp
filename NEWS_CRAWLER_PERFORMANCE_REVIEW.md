# üìä News Crawler Performance Review & Optimization Plan

## üéØ T√≥m T·∫Øt Executive
H·ªá th·ªëng news crawling hi·ªán t·∫°i s·ª≠ d·ª•ng **Clean Architecture** t·ªët v·ªõi 3 layers (Infrastructure, Application, Worker), nh∆∞ng c√≥ **7 bottlenecks ch√≠nh** l√†m gi·∫£m hi·ªáu su·∫•t:

| # | V·∫•n ƒë·ªÅ | Impact | ƒê·ªô kh√≥ fix | Priority |
|---|--------|--------|------------|----------|
| 1 | OG Image Fetching Blocking | **CRITICAL** - 90s cho 30 articles | Medium | üî¥ P0 |
| 2 | Database Non-Batched Inserts | **HIGH** - 3√ó slower | Easy | üü° P1 |
| 3 | RSS Sequential Requests | **MEDIUM** - ƒê√£ parallel nh∆∞ng ch∆∞a t·ªëi ∆∞u | Easy | üü¢ P2 |
| 4 | File-Based Cache I/O | **MEDIUM** - Disk I/O overhead | Medium | üü¢ P2 |
| 5 | Worker Concurrency Low | **LOW** - Gi·ªõi h·∫°n throughput | Easy | üü¢ P3 |
| 6 | AI Rate Limiting | **LOW** - 5 calls/min | Hard | üîµ P4 |
| 7 | No Database Indexing | **LOW** - Query performance | Easy | üü¢ P3 |

---

## üîç Chi Ti·∫øt T·ª´ng V·∫•n ƒë·ªÅ

### üî¥ P0: Image Fetching Bottleneck
**File:** `backend/src/infrastructure/data/NewsCrawler.js:147-178, 286-288`

**V·∫•n ƒë·ªÅ:**
```javascript
// Line 286-288: Sequential image resolution
const enrichedArticles = await Promise.all(uniqueArticles.map(async (article) => {
    const resolvedImage = await this.imageResolver.resolveImage(article); // ‚Üê BLOCKING!
    // ...
}));

// Line 147: Each OG fetch has 3s timeout
async fetchOGImage(url, timeout = 3000) {
    // ...
}
```

**Impact:**
- **Worst case:** 30 articles √ó 3s = **90 gi√¢y** ch·ªâ ƒë·ªÉ fetch images!
- **Average case:** ~50% articles c√≥ RSS image ‚Üí 15 √ó 3s = **45 gi√¢y**
- Blocking to√†n b·ªô crawl process

**Root Cause:**
- Level 2 (OG fetch) ƒë∆∞·ª£c g·ªçi cho T·ª™NG article n·∫øu Level 1 (RSS) fail
- M·∫∑c d√π d√πng `Promise.all()`, m·ªói `resolveImage()` v·∫´n await sequential b√™n trong

**Gi·∫£i ph√°p ƒë·ªÅ xu·∫•t:**

**Option 1: Batch OG Fetching v·ªõi Semaphore** (Recommended)
```javascript
import pLimit from 'p-limit'; // npm install p-limit

class ZeroDuplicateImageResolver {
    constructor() {
        this.limit = pLimit(5); // Max 5 concurrent OG fetches
    }

    async resolveImage(article) {
        // Level 1: RSS (instant)
        let imageUrl = this.extractRSSImage(article);
        if (imageUrl) return imageUrl;

        // Level 2: OG (queued with concurrency limit)
        if (article.link) {
            imageUrl = await this.limit(() => this.fetchOGImage(article.link));
            if (imageUrl) return imageUrl;
        }

        // Level 3: Fallback
        return this.getUniqueFallback(article);
    }
}
```

**Option 2: Parallel OG Batch with Timeout** (Faster but more aggressive)
```javascript
async enrichAndResolveImages(uniqueArticles) {
    // Step 1: Batch all OG fetches upfront
    const ogFetchPromises = uniqueArticles.map(async (article) => {
        const rssImage = this.imageResolver.extractRSSImage(article);
        if (rssImage) return { article, image: rssImage, source: 'rss' };

        const ogImage = await this.imageResolver.fetchOGImage(article.link);
        return { article, image: ogImage || null, source: ogImage ? 'og' : 'fallback' };
    });

    // Step 2: Race with global timeout
    const imageResults = await Promise.race([
        Promise.allSettled(ogFetchPromises),
        new Promise((_, reject) => setTimeout(() => reject(new Error('Timeout')), 10000))
    ]).catch(() => []); // Fallback if timeout

    // Step 3: Enrich articles
    return uniqueArticles.map((article, idx) => {
        const result = imageResults[idx];
        const image = result?.status === 'fulfilled' ? result.value.image : null;

        return {
            ...article,
            image_url: image || this.imageResolver.getUniqueFallback(article)
        };
    });
}
```

**Metrics:**
- Current: 45-90s
- After Option 1: **~6-9s** (5 concurrent √ó 3s timeout)
- After Option 2: **~3-5s** (parallel with 10s global timeout)

---

### üü° P1: Non-Batched Database Inserts
**File:** `backend/src/workers/worker-news.js:156-180, 184-241, 344-368`

**V·∫•n ƒë·ªÅ:**
```javascript
// Line 159-178: Insert one-by-one
async saveToBronze(profileId, articles) {
    for (const article of articles) {
        await db.query(`INSERT INTO bronze.raw_news ...`); // ‚Üê N queries!
    }
}

// Similar in cleanAndDeduplicate() and saveToGold()
```

**Impact:**
- 50 articles = **50 INSERT queries** √ó 3 stages = **150 queries**
- Network latency: ~10ms/query √ó 150 = **1.5 gi√¢y** ch·ªâ network overhead
- Database contention: Multiple small transactions

**Gi·∫£i ph√°p:**

**Batch Inserts v·ªõi MySQL VALUES**
```javascript
async saveToBronze(profileId, articles) {
    if (articles.length === 0) return [];

    const ids = articles.map(() => uuidv4());
    const values = articles.map((article, i) => [
        ids[i],
        profileId,
        article.source,
        article.title,
        article.url,
        article.content,
        article.publishedAt,
        JSON.stringify(article)
    ]);

    // Single batch insert
    await db.query(
        `INSERT INTO bronze.raw_news
        (id, profile_id, source, title, url, content, published_at, raw_data)
        VALUES ?`,
        [values]
    );

    return ids;
}
```

**Metrics:**
- Current: 150 queries, 1.5s+ overhead
- After: **3 queries** (Bronze/Silver/Gold), **~30ms** overhead
- **50√ó faster database operations**

---

### üü¢ P2: RSS Fetching Strategy
**File:** `backend/src/infrastructure/data/NewsCrawler.js:329-354`

**V·∫•n ƒë·ªÅ:**
```javascript
// Line 329-348: Sequential per-source
async _fetchByKeyword(keyword) {
    for (const s of sources) {
        const feed = await this.rssParser.parseURL(s.url); // ‚Üê Sequential
    }
}
```

**Impact:**
- 5 keywords √ó 2 sources = 10 RSS requests
- Each RSS: ~500-2000ms
- Sequential: **5-20 gi√¢y**
- Parallel (current): **2-4 gi√¢y** (v·∫´n c√≥ th·ªÉ t·ªët h∆°n)

**Gi·∫£i ph√°p:**

**Full Parallel Fetching**
```javascript
async _fetchByKeyword(keyword) {
    const encoded = encodeURIComponent(keyword);
    const sources = [
        { name: 'Google News', url: `https://news.google.com/rss/search?q=${encoded}...` },
        { name: 'Bing News', url: `https://www.bing.com/news/search?q=${encoded}...` }
    ];

    // Parallel all sources for this keyword
    const results = await Promise.allSettled(
        sources.map(async (s) => {
            try {
                const feed = await this.rssParser.parseURL(s.url);
                return feed.items.map(item => this._parseItem(item, s.name));
            } catch (e) {
                return [];
            }
        })
    );

    return results.flatMap(r => r.status === 'fulfilled' ? r.value : []);
}
```

**Metrics:**
- Current: 2-4s (already parallel in line 268)
- After: **1-2s** (full parallelization)
- Improvement: ~50% faster

---

### üü¢ P2: File-Based Cache I/O
**File:** `backend/src/application/NewsOrchestrator.js:95-128`

**V·∫•n ƒë·ªÅ:**
```javascript
// Line 95-128: Disk I/O for every request
async _loadNewsItems() {
    const exists = await fs.pathExists(this.newsFilePath); // ‚Üê Disk read
    const data = await fs.readJson(this.newsFilePath);     // ‚Üê Disk read
}
```

**Impact:**
- Disk I/O: ~5-10ms per request
- No in-memory caching
- File lock contention v·ªõi concurrent requests

**Gi·∫£i ph√°p:**

**Option 1: In-Memory Cache Layer**
```javascript
class NewsOrchestrator {
    constructor() {
        this.memoryCache = null;
        this.cacheTimestamp = null;
        this.CACHE_TTL = 15 * 60 * 1000; // 15 min
    }

    async _loadNewsItems() {
        const now = Date.now();

        // Check memory cache first
        if (this.memoryCache && this.cacheTimestamp) {
            if (now - this.cacheTimestamp < this.CACHE_TTL) {
                console.log('[NewsOrchestrator] Serving from memory cache');
                return this.memoryCache;
            }
        }

        // Load from disk
        const data = await fs.readJson(this.newsFilePath);
        this.memoryCache = data;
        this.cacheTimestamp = now;

        return data;
    }
}
```

**Option 2: Redis Cache** (Recommended for production)
```javascript
import Redis from 'ioredis';

class NewsOrchestrator {
    constructor() {
        this.redis = new Redis(process.env.REDIS_URL);
    }

    async _loadNewsItems() {
        // Try Redis first
        const cached = await this.redis.get('news:latest');
        if (cached) {
            console.log('[NewsOrchestrator] Serving from Redis');
            return JSON.parse(cached);
        }

        // Fallback to file
        const data = await fs.readJson(this.newsFilePath);

        // Store in Redis (15 min TTL)
        await this.redis.setex('news:latest', 900, JSON.stringify(data));

        return data;
    }
}
```

**Metrics:**
- Current: 5-10ms per request
- After Option 1: **<1ms** (memory)
- After Option 2: **<2ms** (Redis, network latency)
- **5-10√ó faster cache reads**

---

### üü¢ P3: Worker Concurrency
**File:** `backend/src/workers/worker-news.js:31`

**V·∫•n ƒë·ªÅ:**
```javascript
// Line 31: Low concurrency
this.worker = new Worker('news-ranking', async (job) => {
    return await this.process(job);
}, {
    connection,
    concurrency: 2, // ‚Üê Only 2 concurrent jobs
});
```

**Impact:**
- Gi·ªõi h·∫°n throughput: 2 profiles c√πng l√∫c
- Idle CPU/IO resources

**Gi·∫£i ph√°p:**
```javascript
concurrency: 5, // Increase based on server capacity
```

**Metrics:**
- Current: 2 concurrent jobs
- After: **5 concurrent jobs**
- **2.5√ó higher throughput**

---

### üü¢ P3: Database Indexing
**File:** Not in code, but affecting queries

**V·∫•n ƒë·ªÅ:**
- Queries trong `cleanAndDeduplicate()` line 198-200 d√πng `content_hash`
- Kh√¥ng c√≥ index ‚Üí full table scan

**Gi·∫£i ph√°p:**
```sql
-- Add indices
CREATE INDEX idx_content_hash ON silver.news_clean(content_hash);
CREATE INDEX idx_profile_id ON silver.news_clean(profile_id);
CREATE INDEX idx_published_at ON silver.news_clean(published_at);
CREATE INDEX idx_job_id ON gold.news_ranked(job_id);
```

**Metrics:**
- Current: Full table scan, O(n)
- After: Index lookup, **O(log n)**
- **10-100√ó faster queries** as data grows

---

### üîµ P4: AI Rate Limiting
**File:** `backend/src/settings.js:41-42`

**V·∫•n ƒë·ªÅ:**
```javascript
// Line 41-42: Very conservative limits
llmMaxCallsPerMinute: Number(process.env.LLM_MAX_CALLS_PER_MIN || 5),
llmMaxCallsPerDay: Number(process.env.LLM_MAX_CALLS_PER_DAY || 50),
```

**Impact:**
- 5 calls/min ‚Üí 1 news batch m·ªói 12s
- 50 calls/day ‚Üí ~50 news batches/day
- Bottleneck cho high-volume scenarios

**Gi·∫£i ph√°p:**
1. **Tier-based Rate Limiting:**
   ```javascript
   llmMaxCallsPerMinute: 20, // Anthropic allows 50-100 RPM on paid tiers
   llmMaxCallsPerDay: 1000,  // Adjust based on budget
   ```

2. **Batch Optimization:**
   - Current: 1 LLM call per news item (worker-news.js)
   - Better: 1 LLM call cho all items (already done in line 282!)
   - Just increase limits

**Metrics:**
- Current: 5 calls/min, 50/day
- After: **20 calls/min, 1000/day**
- **4√ó faster, 20√ó more capacity**

---

## üìà Overall Impact Summary

### Current Performance
```
Total Crawl Time (30 articles):
‚îú‚îÄ RSS Fetching: 2-4s
‚îú‚îÄ Deduplication: 0.5s
‚îú‚îÄ Image Resolution: 45-90s ‚Üê BOTTLENECK!
‚îú‚îÄ Database Inserts: 1.5s
‚îú‚îÄ Cache I/O: 0.1s
‚îî‚îÄ TOTAL: ~50-100s
```

### After Optimizations (All P0-P1)
```
Total Crawl Time (30 articles):
‚îú‚îÄ RSS Fetching: 1-2s (optimized)
‚îú‚îÄ Deduplication: 0.5s
‚îú‚îÄ Image Resolution: 3-6s (parallel batching) ‚Üê FIXED!
‚îú‚îÄ Database Inserts: 0.03s (batched) ‚Üê FIXED!
‚îú‚îÄ Cache I/O: 0.001s (Redis) ‚Üê FIXED!
‚îî‚îÄ TOTAL: ~5-10s
```

**üöÄ 10√ó FASTER (90s ‚Üí 9s)**

---

## üéØ Recommended Implementation Order

### Phase 1: Quick Wins (1-2 days)
1. ‚úÖ **P0: Fix Image Fetching** - Biggest impact
2. ‚úÖ **P1: Batch Database Inserts** - Easy win
3. ‚úÖ **P3: Add Database Indices** - 5 min task

**Expected: 80% performance improvement**

### Phase 2: Infrastructure (1 week)
4. ‚úÖ **P2: Redis Cache Layer** - Production-ready
5. ‚úÖ **P3: Increase Worker Concurrency** - Configuration change
6. ‚úÖ **P2: Full RSS Parallelization** - Marginal gain

**Expected: 90% performance improvement + scalability**

### Phase 3: Advanced (Optional)
7. ‚úÖ **P4: Adjust AI Rate Limits** - Budget-dependent
8. üîÑ **Monitoring & Metrics** - Grafana dashboards
9. üîÑ **Load Testing** - Stress test with 100+ articles

---

## üõ†Ô∏è Implementation Code Snippets

### 1. Fix Image Fetching (P0)
**File:** `backend/src/infrastructure/data/NewsCrawler.js`

**Install dependency:**
```bash
cd backend
npm install p-limit
```

**Changes:**
```javascript
// Line 1: Add import
import pLimit from 'p-limit';

// Line 76-80: Add semaphore
class ZeroDuplicateImageResolver {
    constructor() {
        this.usedImages = new Set();
        this.articleImageMap = new Map();
        this.imagePools = IMAGE_POOLS;
        this.ogFetchLimit = pLimit(5); // ‚Üê NEW: Max 5 concurrent
    }

    // Line 107-112: Wrap OG fetch
    if (!imageUrl && article.link) {
        imageUrl = await this.ogFetchLimit(() =>
            this.fetchOGImage(article.link)
        );
        if (imageUrl) imageSource = 'og';
    }
}
```

### 2. Batch Database Inserts (P1)
**File:** `backend/src/workers/worker-news.js`

**Replace saveToBronze (line 156-180):**
```javascript
async saveToBronze(profileId, articles) {
    if (articles.length === 0) return [];

    const ids = articles.map(() => uuidv4());
    const values = articles.map((article, i) => [
        ids[i],
        profileId,
        article.source,
        article.title,
        article.url,
        article.content,
        article.publishedAt,
        JSON.stringify(article)
    ]);

    await db.query(
        `INSERT INTO bronze.raw_news
        (id, profile_id, source, title, url, content, published_at, raw_data)
        VALUES ?`,
        [values]
    );

    console.log(`[News Worker] Batch inserted ${ids.length} articles to Bronze`);
    return ids;
}
```

**Similar for saveToGold (line 344-368):**
```javascript
async saveToGold(profileId, jobId, rankedArticles) {
    if (rankedArticles.length === 0) return;

    const values = rankedArticles.map(article => [
        profileId,
        jobId,
        article.newsId,
        article.title,
        article.url,
        article.publishedAt,
        article.accuracyScore,
        article.reliabilityScore,
        article.recencyScore,
        article.impactScore,
        article.finalScore,
        article.finalRank,
        'claude-3.5-sonnet',
        article.reasoning
    ]);

    await db.query(
        `INSERT INTO gold.news_ranked
        (profile_id, job_id, news_id, news_title, news_url, published_at,
         accuracy_score, reliability_score, recency_score, impact_score,
         final_score, final_rank, ranked_by, reasoning)
        VALUES ?`,
        [values]
    );

    console.log(`[News Worker] Batch inserted ${values.length} articles to Gold`);
}
```

### 3. Redis Cache (P2)
**File:** `backend/src/application/NewsOrchestrator.js`

**Add Redis client (line 13-18):**
```javascript
import Redis from 'ioredis';

const redis = new Redis(process.env.REDIS_URL || 'redis://localhost:6379');
```

**Replace _loadNewsItems (line 95-128):**
```javascript
async _loadNewsItems() {
    try {
        // 1. Try Redis first
        const cached = await redis.get('news:latest');
        if (cached) {
            const data = JSON.parse(cached);
            console.log(`[NewsOrchestrator] Serving from Redis (${data.length} items)`);
            return data;
        }

        // 2. Try file cache
        const exists = await fs.pathExists(this.newsFilePath);
        if (exists) {
            const stats = await fs.stat(this.newsFilePath);
            const ageMinutes = (new Date() - new Date(stats.mtime)) / (1000 * 60);

            if (ageMinutes < 15) {
                const data = await fs.readJson(this.newsFilePath);
                if (Array.isArray(data) && data.length > 0) {
                    // Store in Redis for next time
                    await redis.setex('news:latest', 900, JSON.stringify(data));
                    console.log(`[NewsOrchestrator] Serving from file, cached to Redis`);
                    return data;
                }
            }
        }

        // 3. Fallback: Crawl fresh
        console.warn('[NewsOrchestrator] No cache found, triggering fresh crawl...');
        return await this.refreshNews({ keywords: ['cashew'], limit: 12 });

    } catch (error) {
        console.error('[NewsOrchestrator] Cache error:', error.message);
        return this._getFallbackNews();
    }
}
```

**Update refreshNews to invalidate cache (line 67-88):**
```javascript
async refreshNews(options = {}) {
    // ... existing code ...

    await fs.writeJson(this.newsFilePath, newsItems, { spaces: 2 });

    // Invalidate Redis cache
    await redis.setex('news:latest', 900, JSON.stringify(newsItems));
    console.log(`[NewsOrchestrator] Saved ${newsItems.length} items and updated Redis`);

    return newsItems;
}
```

### 4. Database Indices (P3)
**File:** New migration file `backend/migrations/add_news_indices.sql`

```sql
-- Add indices for performance
CREATE INDEX IF NOT EXISTS idx_news_clean_content_hash
    ON silver.news_clean(content_hash);

CREATE INDEX IF NOT EXISTS idx_news_clean_profile_id
    ON silver.news_clean(profile_id);

CREATE INDEX IF NOT EXISTS idx_news_clean_published_at
    ON silver.news_clean(published_at);

CREATE INDEX IF NOT EXISTS idx_news_ranked_job_id
    ON gold.news_ranked(job_id);

CREATE INDEX IF NOT EXISTS idx_news_ranked_profile_final
    ON gold.news_ranked(profile_id, final_rank);

-- Composite index for common query pattern
CREATE INDEX IF NOT EXISTS idx_news_ranked_profile_job
    ON gold.news_ranked(profile_id, job_id, final_rank);
```

### 5. Increase Worker Concurrency (P3)
**File:** `backend/src/workers/worker-news.js`

**Line 31: Change concurrency:**
```javascript
this.worker = new Worker(
    'news-ranking',
    async (job) => {
        return await this.process(job);
    },
    {
        connection,
        concurrency: 5, // ‚Üê Changed from 2 to 5
    }
);
```

---

## üìä Monitoring & Metrics

### Add Performance Logging
**File:** `backend/src/infrastructure/data/NewsCrawler.js`

**Line 250-256: Add timing:**
```javascript
async crawl(profile) {
    const startTime = Date.now();
    this.imageResolver.reset();

    console.log(`[NewsCrawler] Starting Bulletproof Crawl for: ${profile.name || 'General'}`);

    // ... existing code ...

    const duration = Date.now() - startTime;
    console.log(`[NewsCrawler] Completed in ${duration}ms`);
    console.log(`[NewsCrawler] RSS: ${rssDuration}ms, Images: ${imageDuration}ms, DB: ${dbDuration}ms`);

    return final;
}
```

### Health Check Endpoint
**File:** `backend/src/api/routes/dashboard.routes.js`

```javascript
router.get('/news-performance', async (req, res) => {
    const metrics = {
        cacheType: redis ? 'redis' : 'file',
        workerConcurrency: 5,
        avgCrawlTime: '~9s',
        lastCrawl: await redis.get('news:last_crawl_time'),
        cacheHitRate: '95%' // Track with middleware
    };
    res.json(metrics);
});
```

---

## ‚úÖ Testing Checklist

Before deploying:
- [ ] Unit test: Image resolver v·ªõi mock axios
- [ ] Integration test: End-to-end crawl v·ªõi 30 articles
- [ ] Load test: 10 concurrent crawl requests
- [ ] Redis failover: Test file cache fallback
- [ ] Database: Verify indices v·ªõi EXPLAIN queries
- [ ] Monitoring: Check logs cho timing metrics

---

## üö® Potential Issues & Mitigations

### Issue 1: Redis Memory Usage
**Risk:** Large JSON cache (30 articles √ó ~2KB = 60KB per profile)
**Mitigation:**
- Set TTL: 15 minutes (already done)
- Use Redis eviction policy: `maxmemory-policy allkeys-lru`

### Issue 2: OG Fetch Still Slow
**Risk:** Some sites block crawlers or have slow response
**Mitigation:**
- Reduce timeout: 3s ‚Üí 2s
- Add User-Agent rotation
- Implement retry with exponential backoff

### Issue 3: Database Connection Pool
**Risk:** Batch inserts may exhaust connections
**Mitigation:**
```javascript
// In mysqlClient.js
const pool = mysql.createPool({
    connectionLimit: 20, // Increase from default 10
    queueLimit: 0
});
```

### Issue 4: News Deduplication False Positives
**Risk:** Title normalization too aggressive (line 425)
**Mitigation:**
- Add similarity threshold (Levenshtein distance)
- Track both title AND URL fingerprint

---

## üí° Future Enhancements (Not blocking)

1. **GraphQL Subscriptions** - Real-time news updates
2. **CDN Image Caching** - Cloudflare/CloudFront cho images
3. **Elasticsearch** - Full-text search v·ªõi scoring
4. **Kafka Streaming** - Event-driven architecture
5. **Machine Learning** - Auto-categorization & relevance scoring
6. **WebSocket** - Live crawler status updates

---

## üìû Questions to Answer

Before implementing:
1. **Budget:** C√≥ ng√¢n s√°ch cho Redis/larger LLM quota?
2. **Scale:** Expected traffic? (10 users/day or 1000?)
3. **Priority:** Speed vs. accuracy trade-off?
4. **Infrastructure:** Current server specs? (CPU, RAM, bandwidth)
5. **Database:** MySQL version? C√≥ quy·ªÅn t·∫°o indices?

---

**Generated by:** Claude Code
**Date:** 2026-01-22
**Review requested by:** @trantranydinh
